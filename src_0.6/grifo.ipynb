{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826d7dc9",
   "metadata": {},
   "source": [
    "## Load python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa5739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import gzip\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69296434",
   "metadata": {},
   "source": [
    "## Import information from configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87ccc42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data\n"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "path_data = config.get('paths', 'path_data')\n",
    "\n",
    "print(path_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3ee76",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc55f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to obtain paths to the directories where the fastq files are located.\n",
    "Currently requires demultiplexed data with gzipped files.\n",
    "'''\n",
    "def fastq_directory_paths():\n",
    "    fastq_dir = set()\n",
    "    file_paths = glob.glob(path_data + '/**/*.fastq.gz', recursive=True)\n",
    "    directory_paths = map(os.path.dirname, file_paths)\n",
    "    fastq_dir.update(directory_paths)\n",
    "    return sorted(fastq_dir)\n",
    "\n",
    "'''\n",
    "Function to obtain sample names that are used as base to track the samples through the analysis\n",
    "'''\n",
    "basename = lambda paths: [path.split('/')[-2] if path.endswith('/') else path.split('/')[-1] for path in paths]\n",
    "\n",
    "'''\n",
    "Functions to count the number of raw sequences per sample\n",
    "'''\n",
    "def count_sequences(file_path):\n",
    "    count = 0\n",
    "    with gzip.open(file_path, 'rt') as gz_file:\n",
    "        for line in gz_file:\n",
    "            if line.startswith('@'):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def accumulate_counts(folder_path):\n",
    "    counts = {}\n",
    "    for folder in folder_path:\n",
    "        counts[folder] = 0  # Initialize the folder key with count 0\n",
    "        for file_name in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, file_name)\n",
    "            if file_name.endswith('.fastq.gz'):\n",
    "                counts[folder] += count_sequences(file_path)\n",
    "    return counts\n",
    "\n",
    "'''\n",
    "Wrapper function for NanoFilt\n",
    "'''\n",
    "def filtering(folder_list, minimum_length, maximum_length, qscore, base_list):\n",
    "    for folder, base in zip(folder_list, base_list):\n",
    "        if os.path.exists(os.path.join('results/qc', base)) and os.path.isdir(os.path.join('results/qc', base)):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(os.path.join('results/qc', base))\n",
    "        for file_name in os.listdir(folder):\n",
    "            if file_name.endswith('.fastq.gz'):\n",
    "                file_path = os.path.join(folder, file_name)\n",
    "                file_name_unzipped = file_name[:-3]\n",
    "                command = f'gunzip -c {file_path} | NanoFilt \\\n",
    "                          --length {minimum_length} \\\n",
    "                          --maxlength {maximum_length} \\\n",
    "                          -q {qscore} | gzip > ./results/qc/{base}/{file_name}'\n",
    "                subprocess.run(command, shell=True)\n",
    "                \n",
    "'''\n",
    "Function to concatenate filtered sequences to one file per sample\n",
    "'''\n",
    "def concatenate(bases):\n",
    "    for base in bases:\n",
    "        command = f'cat ./results/qc/{base}/*.fastq.gz > ./results/qc/{base}/{base}_concatenated.fastq.gz'\n",
    "        subprocess.run(command, shell=True)\n",
    "        \n",
    "        command = f'gunzip ./results/qc/{base}/{base}_concatenated.fastq.gz'\n",
    "        subprocess.run(command, shell=True)\n",
    "        \n",
    "        command = f'sed -n \"1~4s/^@/>/p;2~4p\" ./results/qc/{base}/{base}_concatenated.fastq > ./results/qc/{base}/{base}_concatenated.fasta'\n",
    "        subprocess.run(command, shell=True)\n",
    "\n",
    "        #command = f'gzip ./results/qc/{base}/{base}_concatenated.fasta'\n",
    "        #subprocess.run(command, shell=True)\n",
    "        \n",
    "'''\n",
    "Function to count the number of sequences per sample after filtering\n",
    "'''\n",
    "def count_sequences_concat(base_name):\n",
    "    counts = {}\n",
    "    for base in base_name:\n",
    "        counts[base] = 0  # Initialize the count for each base\n",
    "        file_path = './results/qc/' + base + '/' + base + '_concatenated.fasta'\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                if line.startswith('>'):\n",
    "                    counts[base] += 1\n",
    "    return counts\n",
    "\n",
    "'''\n",
    "Wrapper function to generate sequencing summeries using NanoStat and MultiQC\n",
    "'''\n",
    "def stats(base_name):\n",
    "    os.chdir(wdir)\n",
    "    os.makedirs('multiqc')\n",
    "    for base in base_name:\n",
    "        input_file = './results/qc/' + base + '/' + base + '_concatenated.fastq'\n",
    "        output_dir = './multiqc'\n",
    "        output_name = base + \"_statreports\"    \n",
    "        print(input_file)\n",
    "        print(output_dir)\n",
    "        command = [\n",
    "            \"NanoStat\",\n",
    "            \"--fastq\",\n",
    "            input_file,\n",
    "            \"--outdir\", output_dir,\n",
    "            \"--name\", output_name\n",
    "        ]   \n",
    "    \n",
    "        subprocess.run(command)\n",
    "        os.chdir(wdir)\n",
    "    os.chdir('./multiqc')\n",
    "    command = [\n",
    "        \"multiqc\",\n",
    "        \".\"\n",
    "    ]\n",
    "    subprocess.run(command)\n",
    "    os.chdir(wdir)\n",
    "\n",
    "'''\n",
    "Function to convert sequence files in fasta format to csv\n",
    "'''\n",
    "def fasta2csv(base_name):\n",
    "    for base in base_name:\n",
    "        fasta = './results/qc/' + base + '/' + base + '_concatenated.fasta'\n",
    "        output = './results/qc/' + base + '/' + base + '_concatenated.csv'\n",
    "\n",
    "        out_lines = []\n",
    "        temp_line = ''\n",
    "        with open(fasta, 'r') as fp:\n",
    "            for line in fp:\n",
    "                if line.startswith('>'):\n",
    "                    out_lines.append(temp_line)\n",
    "                    temp_line = line.strip() + ','\n",
    "                else:\n",
    "                    temp_line += line.strip()\n",
    "        out_lines.append(temp_line)\n",
    "\n",
    "        with open(output, 'w') as fp_out:\n",
    "            fp_out.write('id,sequence' + '\\n'.join(out_lines))\n",
    "            \n",
    "'''\n",
    "Wrapper function to run the ashure clustering algorithm\n",
    "'''\n",
    "def cluster(base):\n",
    "    os.chdir(wdir)\n",
    "    shutil.copy('./ashure.py', './results/qc/' + base)\n",
    "    shutil.copy('./bilge_pype.py', './results/qc/' + base)\n",
    "    os.chdir('./results/qc/' + base)\n",
    "    script_path = \"./ashure.py\"\n",
    "    input_file = base + \"_concatenated.csv\"\n",
    "    output_file = base + \"_clusters.csv\"    \n",
    "    \n",
    "    command = [\n",
    "        script_path,\n",
    "        \"clst\",\n",
    "        \"-i\", input_file,\n",
    "        \"-o\", output_file,\n",
    "        \"-iter\", config.get('ashure', 'niter'),\n",
    "        \"-r\"\n",
    "    ]   \n",
    "    \n",
    "    subprocess.run(command)\n",
    "    os.chdir(wdir)\n",
    "\n",
    "'''\n",
    "Function to convert sequence files in csv format to fasta\n",
    "'''\n",
    "def csv2fasta(base_name):\n",
    "    for base in base_name:\n",
    "        csv_path = './results/qc/' + base + '/' + base + '_clusters.csv'\n",
    "        output_path = './results/qc/' + base + '/' + base + '_clusters.fasta'\n",
    "\n",
    "        if os.path.exists(csv_path):\n",
    "            out_lines = []\n",
    "            temp_line = ''\n",
    "            with open(csv_path, 'r') as csv_file:\n",
    "                for line in csv_file:\n",
    "                    cols = line.split(\",\")\n",
    "                    out_lines.append(temp_line)\n",
    "                    temp_line = \">\" + cols[0] + \"\\n\" + cols[1] + \"\\n\"\n",
    "\n",
    "            out_lines.append(temp_line)\n",
    "\n",
    "            with open(output_path, 'w') as csv_out:\n",
    "                csv_out.write(''.join(out_lines)[13:])\n",
    "                \n",
    "'''\n",
    "Wrapper function to run cutadapt\n",
    "'''\n",
    "def remove_primers(base_name):\n",
    "    for base in base_name:\n",
    "        file_path = './results/qc/' + base + '/' + base + '_clusters.fasta'\n",
    "        out_path = './results/qc/' + base + '/' + base + '_clusters_cut.fasta'\n",
    "        if os.path.exists(file_path):\n",
    "            command = [\n",
    "                'cutadapt',\n",
    "                '-a', 'CAGCAGCCGCGGTAATTCC;max_error_rate=0.20',\n",
    "                '-g', 'CCCGTGTTGAGTCAAATTAAGC;max_error_rate=0.20',\n",
    "                '--revcomp',\n",
    "                '-o', out_path,\n",
    "                file_path\n",
    "            ]\n",
    "            subprocess.run(command)\n",
    "\n",
    "'''\n",
    "Wrapper function to run blastn\n",
    "'''                        \n",
    "def blast(base_name):\n",
    "    for base in base_name:\n",
    "        file_path = './results/qc/' + base + '/' + base + '_clusters_cut.fasta'\n",
    "        db = config.get('BLAST', 'db')\n",
    "        if os.path.exists(file_path):\n",
    "            print(\"Running blastn on\", base)\n",
    "            output_csv = './results/qc/' + base + '/' + base + '_' + db + '_blastn.csv'\n",
    "\n",
    "            command = [\n",
    "                \"blastn\",\n",
    "                \"-db\", config.get('paths', 'path_to_blastdb'),\n",
    "                \"-query\", file_path,\n",
    "                \"-task\", \"blastn\",\n",
    "                \"-dust\", \"no\",\n",
    "                \"-num_threads\", str(config.get('BLAST', 'numthreads')),\n",
    "                \"-outfmt\", \"7 delim=, sseqid stitle qacc sacc evalue bitscore length pident\",\n",
    "                \"-max_target_seqs\", str(config.get('BLAST', 'mts')),\n",
    "                \"-perc_identity\", str(config.get('BLAST', 'pct_ident')),\n",
    "                \"-out\", output_csv\n",
    "            ]\n",
    "\n",
    "            subprocess.run(command)\n",
    "\n",
    "'''\n",
    "Function to handle the blastn output files and generate a concatenated table with the taxonomic annotations\n",
    "'''\n",
    "def make_output_file(base_name):\n",
    "    db = config.get('BLAST', 'db')\n",
    "    for base in base_name:\n",
    "        input_csv = './results/qc/' + base + '/' + base + '_' + db + '_blastn.csv'\n",
    "        output_csv = './results/qc/' + base + '/' + base + '_' + db + '_blastn2.csv'\n",
    "        if os.path.exists(input_csv):\n",
    "            with open(input_csv, 'r') as infile, open(output_csv, 'w') as outfile:\n",
    "                for line in infile:\n",
    "                    if not line.startswith('#'):\n",
    "                        outfile.write(line)\n",
    "                    \n",
    "    for base in base_name:\n",
    "        input_csv = './results/qc/' + base + '/' + base + '_' + db + '_blastn2.csv'\n",
    "        print(input_csv)\n",
    "        output_csv = './results/qc/' + base + '/' + base + '_' + db + '_ASV.csv'\n",
    "        if os.path.exists(input_csv) and os.path.getsize(input_csv) > 0:\n",
    "            # load file\n",
    "            df = pd.read_csv(input_csv, sep=',')\n",
    "\n",
    "            # add column names\n",
    "            df.columns=['accession', 'taxonomic_annotation', 'cluster', 'accession', 'evalue', 'bitscore', 'alignment_length', 'percentage_identity']\n",
    "\n",
    "            # select only rows with alignment length >= 500 bp\n",
    "            df2 = df[df['alignment_length'] >= 500]\n",
    "\n",
    "            # arrange rows by match percentage\n",
    "            df3 = df2.sort_values(by=['percentage_identity'], ascending=False)\n",
    "\n",
    "            # keep only first row of each ASV\n",
    "            df4 = df3.drop_duplicates(subset=['cluster'], keep='first', inplace=False, ignore_index=False)\n",
    "\n",
    "            # add sample name information\n",
    "            df4['#sample_name'] = base\n",
    "\n",
    "            df4['taxonomy'] = df4['taxonomic_annotation'].replace('\"', '')\n",
    "\n",
    "            df5 = df4[['#sample_name', 'cluster', 'accession', 'evalue', 'bitscore', 'alignment_length', 'percentage_identity', 'taxonomic_annotation']]\n",
    "\n",
    "            df5.to_csv(output_csv, sep=';', index=False, header=False)\n",
    "\n",
    "    intermediate = '_' + db + '_eDNA.csv'\n",
    "    final = db + '_eDNA.csv'\n",
    "    if os.path.exists(intermediate):\n",
    "        os.remove(intermediate)\n",
    "    else:\n",
    "        pass\n",
    "    try:\n",
    "        with open(intermediate, \"w\") as f:\n",
    "            f.write(\"\")\n",
    "    except OSError as e:\n",
    "        rint(f\"Error creating file: {e}\")\n",
    "    for base in base_name:\n",
    "        file_path = './results/qc/' + base + '/' + base + '_' + db + '_ASV.csv'\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, \"r\") as input_file, open(intermediate, \"a\") as output_file:\n",
    "                output_file.write(input_file.read())\n",
    "\n",
    "    with open(intermediate, \"r\") as input_file, open(final, \"w\") as output_file:\n",
    "        output_file.write(\"counts,cluster,accession,accession,evalue,bitscore,alignment_length,percentage_identity,taxonomic_annotation\\n\")\n",
    "        for line in input_file:\n",
    "            if not line.startswith(\"#\"):\n",
    "                output_file.write(line.replace(\";\", \",\").replace(\"|\", \",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168d0517",
   "metadata": {},
   "source": [
    "## Actual execution of the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac48b62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/barcode01', '../data/barcode02', '../data/barcode03']\n",
      "['barcode01', 'barcode02', 'barcode03']\n",
      "{'../data/barcode01': 28513, '../data/barcode02': 28554, '../data/barcode03': 28469}\n",
      "{'barcode01': 25040, 'barcode02': 26721, 'barcode03': 20929}\n",
      "/home/pascal/Documents/git_projects/grifo/src_0.6\n",
      "./results/qc/barcode01/barcode01_concatenated.fastq\n",
      "./multiqc\n",
      "./results/qc/barcode02/barcode02_concatenated.fastq\n",
      "./multiqc\n",
      "./results/qc/barcode03/barcode03_concatenated.fastq\n",
      "./multiqc\n",
      "\n",
      "\u001b[38;5;208m///\u001b[0m \u001b]8;id=208268;https://multiqc.info\u001b\\\u001b[1mMultiQC\u001b[0m\u001b]8;;\u001b\\ 🔍 \u001b[2mv1.22.2\u001b[0m\n",
      "\n",
      "\u001b[34m       file_search\u001b[0m | Search path: /home/pascal/Documents/git_projects/grifo/src_0.6/multiqc\n",
      "\u001b[2K         \u001b[34msearching\u001b[0m | \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[32m3/3\u001b[0m    \n",
      "\u001b[?25h\u001b[34m          nanostat\u001b[0m | Found 3 reports\n",
      "\u001b[34m     write_results\u001b[0m | Data        : multiqc_data\n",
      "\u001b[34m     write_results\u001b[0m | Report      : multiqc_report.html\n",
      "\u001b[34m           multiqc\u001b[0m | MultiQC complete\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: check_toolchain: /home/pascal/anaconda3/envs/GEANS/bin/minimap2 found\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: check_toolchain: /usr/bin/bwa found\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: check_toolchain: /usr/bin/bowtie2 found\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: check_toolchain: /home/pascal/anaconda3/envs/GEANS/bin/spoa found\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: check_toolchain: /usr/bin/mafft found\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: Running the pipeline\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: minimap2_path = minimap2\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: spoa_path     = spoa\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: config_file = \n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: primer_file = primers.csv\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: prfg_fs     = 500-1200\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: prfg_config = -k5 -w1 -s 20 -P\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: prfg_pmr_thresh = 10\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: db_file     = pseudodb.csv.gz\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: N fastq     = 0\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: frag_folder = ./frags/\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: fgs_config  = -k10 -w1\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: msa_folder  = ./msa/\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: msa_config  = -n -15 -g -10 -l 0 -r 0\n",
      "pid[11983] 2024-06-05 21:28:49.016 INFO: check_toolchain: /home/pascal/anaconda3/envs/GEANS/bin/minimap2 found\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: msa_metric  = AS\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: msa_thresh  = 50\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: msa_batch_size   = 100\n",
      "pid[11984] 2024-06-05 21:28:49.016 INFO: msa_gap_thresh   = 4\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: msa_padding      = 0\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: check_toolchain: /usr/bin/bwa found\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: msa_thread_lock  = False\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: cons_file        = consensus.csv.gz\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: fpmr_config      = -k5 -w1 -s 20 -P\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: fpmr_thresh      = 10\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: check_toolchain: /usr/bin/bowtie2 found\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: pmatch_file      = pmatch.csv.gz\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: cin_file         = barcode02_concatenated.csv\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: cout_file        = barcode02_clusters.csv\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: check_toolchain: /home/pascal/anaconda3/envs/GEANS/bin/spoa found\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: clst_init        = \n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: clst_folder      = ./clusters/\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: clst_pw_config   = -k15 -w10 -p 0.9 -D\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: check_toolchain: /usr/bin/mafft found\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: clst_N      = 2000\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: Running the pipeline\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: clst_th_s   = 4\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: minimap2_path = minimap2\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: clst_th_m   = 0.9\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: spoa_path     = spoa\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: clst_min_k  = 5\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: config_file = \n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: clst_csize  = 20\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: primer_file = primers.csv\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: clst_N_iter = 5\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: prfg_fs     = 500-1200\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: workspace        = ./workspace/\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: prfg_config = -k5 -w1 -s 20 -P\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: Run Pseudoref generator = False\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: prfg_pmr_thresh = 10\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: Run Fragment Search     = False\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: db_file     = pseudodb.csv.gz\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: Run Multi-Seq Alignment = False\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: N fastq     = 0\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: Run Consensus           = False\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: frag_folder = ./frags/\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: Run primer match        = False\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: fgs_config  = -k10 -w1\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: Run trim primers        = False\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: msa_folder  = ./msa/\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: Run clustering          = True\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: msa_config  = -n -15 -g -10 -l 0 -r 0\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: low_mem                 = False\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: msa_metric  = AS\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: Loading cin_file: barcode02_concatenated.csv\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: msa_thresh  = 50\n",
      "pid[11984] 2024-06-05 21:28:49.017 INFO: Loading barcode02_concatenated.csv\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: msa_batch_size   = 100\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: msa_gap_thresh   = 4\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: msa_padding      = 0\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: msa_thread_lock  = False\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: cons_file        = consensus.csv.gz\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: fpmr_config      = -k5 -w1 -s 20 -P\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: fpmr_thresh      = 10\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: pmatch_file      = pmatch.csv.gz\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: cin_file         = barcode01_concatenated.csv\n",
      "pid[11983] 2024-06-05 21:28:49.017 INFO: cout_file        = barcode01_clusters.csv\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: clst_init        = \n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: clst_folder      = ./clusters/\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: clst_pw_config   = -k15 -w10 -p 0.9 -D\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: clst_N      = 2000\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: clst_th_s   = 4\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: clst_th_m   = 0.9\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: clst_min_k  = 5\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: clst_csize  = 20\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: clst_N_iter = 5\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: workspace        = ./workspace/\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: Run Pseudoref generator = False\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: Run Fragment Search     = False\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: Run Multi-Seq Alignment = False\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: Run Consensus           = False\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: Run primer match        = False\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: Run trim primers        = False\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: Run clustering          = True\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: low_mem                 = False\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: Loading cin_file: barcode01_concatenated.csv\n",
      "pid[11983] 2024-06-05 21:28:49.018 INFO: Loading barcode01_concatenated.csv\n",
      "pid[11982] 2024-06-05 21:28:49.018 INFO: check_toolchain: /home/pascal/anaconda3/envs/GEANS/bin/minimap2 found\n",
      "pid[11982] 2024-06-05 21:28:49.018 INFO: check_toolchain: /usr/bin/bwa found\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: check_toolchain: /usr/bin/bowtie2 found\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: check_toolchain: /home/pascal/anaconda3/envs/GEANS/bin/spoa found\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: check_toolchain: /usr/bin/mafft found\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: Running the pipeline\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: minimap2_path = minimap2\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: spoa_path     = spoa\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: config_file = \n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: primer_file = primers.csv\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: prfg_fs     = 500-1200\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: prfg_config = -k5 -w1 -s 20 -P\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: prfg_pmr_thresh = 10\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: db_file     = pseudodb.csv.gz\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: N fastq     = 0\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: frag_folder = ./frags/\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: fgs_config  = -k10 -w1\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: msa_folder  = ./msa/\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: msa_config  = -n -15 -g -10 -l 0 -r 0\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: msa_metric  = AS\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: msa_thresh  = 50\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: msa_batch_size   = 100\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: msa_gap_thresh   = 4\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: msa_padding      = 0\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: msa_thread_lock  = False\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: cons_file        = consensus.csv.gz\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: fpmr_config      = -k5 -w1 -s 20 -P\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: fpmr_thresh      = 10\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: pmatch_file      = pmatch.csv.gz\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: cin_file         = barcode03_concatenated.csv\n",
      "pid[11982] 2024-06-05 21:28:49.019 INFO: cout_file        = barcode03_clusters.csv\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: clst_init        = \n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: clst_folder      = ./clusters/\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: clst_pw_config   = -k15 -w10 -p 0.9 -D\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: clst_N      = 2000\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: clst_th_s   = 4\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: clst_th_m   = 0.9\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: clst_min_k  = 5\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: clst_csize  = 20\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: clst_N_iter = 5\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: workspace        = ./workspace/\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: Run Pseudoref generator = False\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: Run Fragment Search     = False\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: Run Multi-Seq Alignment = False\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: Run Consensus           = False\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: Run primer match        = False\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: Run trim primers        = False\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: Run clustering          = True\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: low_mem                 = False\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: Loading cin_file: barcode03_concatenated.csv\n",
      "pid[11982] 2024-06-05 21:28:49.020 INFO: Loading barcode03_concatenated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"./ashure.py\", line 1169, in <module>\n",
      "    main()\n",
      "  File \"./ashure.py\", line 1157, in main\n",
      "    data, flist = perform_cluster(df, df_d, max_iter=config['clst_N_iter'], csize=config['clst_csize'], N=config['clst_N'], th_s=config['clst_th_s'], th_m=config['clst_th_m'], pw_config=config['clst_pw_config'], msa_config=config['msa_config'], workspace=config['clst_folder'], track_file=config['clst_iter_out'], timestamp=True)\n",
      "  File \"./ashure.py\", line 561, in perform_cluster\n",
      "    df_c = bpy.cluster_sample(df_q, N=5, th=0.8, rsize=1, config=pw_config, workspace=workspace)\n",
      "  File \"/home/pascal/Documents/git_projects/grifo/src_0.6/results/qc/barcode03/bilge_pype.py\", line 1351, in cluster_sample\n",
      "    df_c.append(df_q[df_q['id'].isin(qry[ridx[:rsize]])][['id','sequence']])\n",
      "  File \"/home/pascal/anaconda3/envs/GEANS/lib/python3.8/site-packages/pandas/core/generic.py\", line 5989, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'DataFrame' object has no attribute 'append'\n",
      "Traceback (most recent call last):\n",
      "  File \"./ashure.py\", line 1169, in <module>\n",
      "    main()\n",
      "  File \"./ashure.py\", line 1157, in main\n",
      "    data, flist = perform_cluster(df, df_d, max_iter=config['clst_N_iter'], csize=config['clst_csize'], N=config['clst_N'], th_s=config['clst_th_s'], th_m=config['clst_th_m'], pw_config=config['clst_pw_config'], msa_config=config['msa_config'], workspace=config['clst_folder'], track_file=config['clst_iter_out'], timestamp=True)\n",
      "  File \"./ashure.py\", line 561, in perform_cluster\n",
      "    df_c = bpy.cluster_sample(df_q, N=5, th=0.8, rsize=1, config=pw_config, workspace=workspace)\n",
      "  File \"/home/pascal/Documents/git_projects/grifo/src_0.6/results/qc/barcode01/bilge_pype.py\", line 1351, in cluster_sample\n",
      "    df_c.append(df_q[df_q['id'].isin(qry[ridx[:rsize]])][['id','sequence']])\n",
      "  File \"/home/pascal/anaconda3/envs/GEANS/lib/python3.8/site-packages/pandas/core/generic.py\", line 5989, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'DataFrame' object has no attribute 'append'\n",
      "Traceback (most recent call last):\n",
      "  File \"./ashure.py\", line 1169, in <module>\n",
      "    main()\n",
      "  File \"./ashure.py\", line 1157, in main\n",
      "    data, flist = perform_cluster(df, df_d, max_iter=config['clst_N_iter'], csize=config['clst_csize'], N=config['clst_N'], th_s=config['clst_th_s'], th_m=config['clst_th_m'], pw_config=config['clst_pw_config'], msa_config=config['msa_config'], workspace=config['clst_folder'], track_file=config['clst_iter_out'], timestamp=True)\n",
      "  File \"./ashure.py\", line 561, in perform_cluster\n",
      "    df_c = bpy.cluster_sample(df_q, N=5, th=0.8, rsize=1, config=pw_config, workspace=workspace)\n",
      "  File \"/home/pascal/Documents/git_projects/grifo/src_0.6/results/qc/barcode02/bilge_pype.py\", line 1351, in cluster_sample\n",
      "    df_c.append(df_q[df_q['id'].isin(qry[ridx[:rsize]])][['id','sequence']])\n",
      "  File \"/home/pascal/anaconda3/envs/GEANS/lib/python3.8/site-packages/pandas/core/generic.py\", line 5989, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'DataFrame' object has no attribute 'append'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/qc/barcode01/barcode01_SILVA_138_blastn2.csv\n",
      "./results/qc/barcode02/barcode02_SILVA_138_blastn2.csv\n",
      "./results/qc/barcode03/barcode03_SILVA_138_blastn2.csv\n"
     ]
    }
   ],
   "source": [
    "# Load paths to directories where fastq files are located\n",
    "fastq_dir = fastq_directory_paths()\n",
    "print(fastq_dir)\n",
    "\n",
    "# Extract sample names from data\n",
    "base_name = basename(fastq_dir)\n",
    "print(base_name)\n",
    "\n",
    "# Count number of raw sequences per sample\n",
    "print(accumulate_counts(fastq_dir))\n",
    "\n",
    "# Filter sequences using NanoFilt\n",
    "filtering(fastq_dir, config.get('NanoFilt', 'minlength'), config.get('NanoFilt', 'maxlength'), config.get('NanoFilt', 'qscore'), base_name)\n",
    "\n",
    "# Concatenate filtered sequences to one file per sample\n",
    "concatenate(base_name)\n",
    "        \n",
    "# Count number of sequences per sample after filtering\n",
    "print(count_sequences_concat(base_name))\n",
    "\n",
    "# Make sure you save the original working directory before using moving around directories\n",
    "wdir = os.getcwd()\n",
    "print(wdir)\n",
    "\n",
    "# Run NanoStat\n",
    "stats(base_name)\n",
    "\n",
    "# Convert fasta files to csv\n",
    "fasta2csv(base_name)\n",
    "\n",
    "# Run the actual clustering\n",
    "if __name__ == '__main__':\n",
    "    num_processes = 8  # Number of available CPU cores \n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        pool.map(cluster, base_name)\n",
    "\n",
    "# Move back to original working directory after clustering\n",
    "os.chdir(wdir)\n",
    "\n",
    "# Convert csv files to fasta\n",
    "csv2fasta(base_name)\n",
    "\n",
    "# Remove primers using a wrapper function for cutadapt\n",
    "remove_primers(base_name)\n",
    "\n",
    "# Taxonomic annotation using blastn\n",
    "blast(base_name)\n",
    "\n",
    "# Handle the blastn output files and generate a concatenated table with the taxonomic annotations\n",
    "make_output_file(base_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a5e3a7",
   "metadata": {},
   "source": [
    "## Taxonomic assignment using PR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f909d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running blastn on barcode01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: [blastn] Query_174 cluster173: Sequence contains no data \n",
      "Warning: [blastn] Query_575 cluster574 rc: Sequence contains no data \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running blastn on barcode02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: [blastn] Query_191 cluster190: Sequence contains no data \n",
      "Warning: [blastn] Query_348 cluster347: Sequence contains no data \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running blastn on barcode03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: [blastn] Query_123 cluster122: Sequence contains no data \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running blastn on barcode04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: [blastn] Query_113 cluster112: Sequence contains no data \n",
      "Warning: [blastn] Query_128 cluster127: Sequence contains no data \n",
      "Warning: [blastn] Query_143 cluster142: Sequence contains no data \n",
      "Warning: [blastn] Query_157 cluster156: Sequence contains no data \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running blastn on barcode05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: [blastn] Query_350 cluster349: Sequence contains no data \n",
      "Warning: [blastn] Query_378 cluster377: Sequence contains no data \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running blastn on barcode06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: [blastn] Query_174 cluster173 rc: Sequence contains no data \n",
      "Warning: [blastn] Query_252 cluster251: Sequence contains no data \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running blastn on barcode07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: [blastn] Query_32 cluster31: Sequence contains no data \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running blastn on barcode08\n",
      "Running blastn on barcode09\n",
      "Running blastn on barcode10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: [blastn] Query_161 cluster160: Sequence contains no data \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running blastn on barcode11\n",
      "Running blastn on barcode12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: [blastn] Query_293 cluster292: Sequence contains no data \n"
     ]
    }
   ],
   "source": [
    "path_to_blastdb = \"/home/pascal/Documents/GEANS/eDNA_18S/PR2_5.0.0/pr2_version_5.0.0_SSU_taxo_long.fasta\"\n",
    "numthreads = 8\n",
    "mts = 10\n",
    "pct_ident = 90\n",
    "db = 'PR2'\n",
    "\n",
    "for base in base_name:\n",
    "    file_path = './results/qc/' + base + '/' + base + '_clusters_cut.fasta'\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"Running blastn on\", base)\n",
    "        output_csv = './results/qc/' + base + '/' + base + '_' + db + '_blastn.csv'\n",
    "\n",
    "        command = [\n",
    "            \"blastn\",\n",
    "            \"-db\", path_to_blastdb,\n",
    "            \"-query\", file_path,\n",
    "            \"-task\", \"blastn\",\n",
    "            \"-dust\", \"no\",\n",
    "            \"-num_threads\", str(numthreads),\n",
    "            \"-outfmt\", \"7 delim=, sseqid stitle qacc sacc evalue bitscore length pident\",\n",
    "            \"-max_target_seqs\", str(mts),\n",
    "            \"-perc_identity\", str(pct_ident),\n",
    "            \"-out\", output_csv\n",
    "        ]\n",
    "\n",
    "        subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23cb23fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = 'PR2'\n",
    "\n",
    "\n",
    "for base in base_name:\n",
    "    input_csv = './results/qc/' + base + '/' + base + '_' + db + '_blastn.csv'\n",
    "    output_csv = './results/qc/' + base + '/' + base + '_' + db + '_blastn2.csv'\n",
    "    if os.path.exists(input_csv):\n",
    "        with open(input_csv, 'r') as infile, open(output_csv, 'w') as outfile:\n",
    "            for line in infile:\n",
    "                if not line.startswith('#'):\n",
    "                    outfile.write(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8946200",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = 'PR2'\n",
    "\n",
    "for base in base_name:\n",
    "    input_csv = './results/qc/' + base + '/' + base + '_' + db + '_blastn.csv'\n",
    "    output_csv = './results/qc/' + base + '/' + base + '_' + db + '_blastn2.csv'\n",
    "    if os.path.exists(input_csv):\n",
    "        with open(input_csv, 'r') as infile, open(output_csv, 'w') as outfile:\n",
    "            for line in infile:\n",
    "                if not line.startswith('#'):\n",
    "                    comma_count = line.count(',')\n",
    "                    if comma_count == 10:\n",
    "                        comma_indices = []\n",
    "                        for i, char in enumerate(line):\n",
    "                            if char == ',':\n",
    "                                comma_indices.append(i)\n",
    "\n",
    "                        if len(comma_indices) >= 6:\n",
    "                            line = line[:comma_indices[0]] + line[comma_indices[0]+1:]\n",
    "                            line = line[:comma_indices[2]-1] + line[comma_indices[2]:]\n",
    "                            line = line[:comma_indices[5]-2] + line[comma_indices[5]-1:]\n",
    "                    elif comma_count == 13:\n",
    "                        comma_indices = []\n",
    "                        for i, char in enumerate(line):\n",
    "                            if char == ',':\n",
    "                                comma_indices.append(i)\n",
    "\n",
    "                        indices_to_remove = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  # Create a list to track the indices to remove\n",
    "\n",
    "                        if len(comma_indices) >= 13:\n",
    "                            indices_to_remove[0] = 1  # Mark the first comma to remove\n",
    "                            indices_to_remove[1] = 1  # Mark the second comma to remove\n",
    "                            indices_to_remove[3] = 1  # Mark the fourth comma to remove\n",
    "                            indices_to_remove[4] = 1  # Mark the fifth comma to remove\n",
    "                            indices_to_remove[7] = 1  # Mark the eighth comma to remove\n",
    "                            indices_to_remove[8] = 1  # Mark the ninth comma to remove\n",
    "\n",
    "                        updated_line = \"\"\n",
    "                        for i, char in enumerate(line):\n",
    "                            if char == ',' and indices_to_remove[comma_indices.index(i)] == 1:\n",
    "                                continue\n",
    "                            updated_line += char\n",
    "\n",
    "                        line = updated_line\n",
    "                    outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed8c501a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/qc/barcode01/barcode01_PR2_blastn2.csv\n",
      "./results/qc/barcode02/barcode02_PR2_blastn2.csv\n",
      "./results/qc/barcode03/barcode03_PR2_blastn2.csv\n",
      "./results/qc/barcode04/barcode04_PR2_blastn2.csv\n",
      "./results/qc/barcode05/barcode05_PR2_blastn2.csv\n",
      "./results/qc/barcode06/barcode06_PR2_blastn2.csv\n",
      "./results/qc/barcode07/barcode07_PR2_blastn2.csv\n",
      "./results/qc/barcode08/barcode08_PR2_blastn2.csv\n",
      "./results/qc/barcode09/barcode09_PR2_blastn2.csv\n",
      "./results/qc/barcode10/barcode10_PR2_blastn2.csv\n",
      "./results/qc/barcode11/barcode11_PR2_blastn2.csv\n",
      "./results/qc/barcode12/barcode12_PR2_blastn2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1122442/2543769090.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['#sample_name'] = base\n",
      "/tmp/ipykernel_1122442/2543769090.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['taxonomy'] = df4['taxonomic_annotation'].replace('\"', '')\n",
      "/tmp/ipykernel_1122442/2543769090.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['#sample_name'] = base\n",
      "/tmp/ipykernel_1122442/2543769090.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['taxonomy'] = df4['taxonomic_annotation'].replace('\"', '')\n",
      "/tmp/ipykernel_1122442/2543769090.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['#sample_name'] = base\n",
      "/tmp/ipykernel_1122442/2543769090.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['taxonomy'] = df4['taxonomic_annotation'].replace('\"', '')\n",
      "/tmp/ipykernel_1122442/2543769090.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['#sample_name'] = base\n",
      "/tmp/ipykernel_1122442/2543769090.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['taxonomy'] = df4['taxonomic_annotation'].replace('\"', '')\n",
      "/tmp/ipykernel_1122442/2543769090.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['#sample_name'] = base\n",
      "/tmp/ipykernel_1122442/2543769090.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['taxonomy'] = df4['taxonomic_annotation'].replace('\"', '')\n",
      "/tmp/ipykernel_1122442/2543769090.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['#sample_name'] = base\n",
      "/tmp/ipykernel_1122442/2543769090.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['taxonomy'] = df4['taxonomic_annotation'].replace('\"', '')\n",
      "/tmp/ipykernel_1122442/2543769090.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['#sample_name'] = base\n",
      "/tmp/ipykernel_1122442/2543769090.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['taxonomy'] = df4['taxonomic_annotation'].replace('\"', '')\n",
      "/tmp/ipykernel_1122442/2543769090.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['#sample_name'] = base\n",
      "/tmp/ipykernel_1122442/2543769090.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['taxonomy'] = df4['taxonomic_annotation'].replace('\"', '')\n",
      "/tmp/ipykernel_1122442/2543769090.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['#sample_name'] = base\n",
      "/tmp/ipykernel_1122442/2543769090.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['taxonomy'] = df4['taxonomic_annotation'].replace('\"', '')\n",
      "/tmp/ipykernel_1122442/2543769090.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['#sample_name'] = base\n",
      "/tmp/ipykernel_1122442/2543769090.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['taxonomy'] = df4['taxonomic_annotation'].replace('\"', '')\n",
      "/tmp/ipykernel_1122442/2543769090.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['#sample_name'] = base\n",
      "/tmp/ipykernel_1122442/2543769090.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['taxonomy'] = df4['taxonomic_annotation'].replace('\"', '')\n",
      "/tmp/ipykernel_1122442/2543769090.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['#sample_name'] = base\n",
      "/tmp/ipykernel_1122442/2543769090.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4['taxonomy'] = df4['taxonomic_annotation'].replace('\"', '')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db = 'PR2'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for base in base_name:\n",
    "    input_csv = './results/qc/' + base + '/' + base + '_' + db + '_blastn2.csv'\n",
    "    print(input_csv)\n",
    "    output_csv = './results/qc/' + base + '/' + base + '_' + db + '_ASV.csv'\n",
    "    if os.path.exists(input_csv) and os.path.getsize(input_csv) > 0:\n",
    "        # load file\n",
    "        df = pd.read_csv(input_csv, sep=',')\n",
    "    \n",
    "        # add column names\n",
    "        df.columns=['accession', 'taxonomic_annotation', 'cluster', 'accession', 'evalue', 'bitscore', 'alignment_length', 'percentage_identity']\n",
    "\n",
    "        # select only rows with alignment length >= 500 bp\n",
    "        df2 = df[df['alignment_length'] >= 500]\n",
    "\n",
    "        # arrange rows by match percentage\n",
    "        df3 = df2.sort_values(by=['percentage_identity'], ascending=False)\n",
    "\n",
    "        # keep only first row of each ASV\n",
    "        df4 = df3.drop_duplicates(subset=['cluster'], keep='first', inplace=False, ignore_index=False)\n",
    "\n",
    "        # add sample name information\n",
    "        df4['#sample_name'] = base\n",
    "\n",
    "        df4['taxonomy'] = df4['taxonomic_annotation'].replace('\"', '')\n",
    "\n",
    "        df5 = df4[['#sample_name', 'cluster', 'accession', 'evalue', 'bitscore', 'alignment_length', 'percentage_identity', 'taxonomic_annotation']]\n",
    "\n",
    "\n",
    "        df5.to_csv(output_csv, sep=';', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dcbbe910",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"_PR2_eDNA.csv\"):\n",
    "    os.remove(\"_PR2_eDNA.csv\")\n",
    "\n",
    "for base in base_name:\n",
    "    file_path = './results/qc/' + base + '/' + base + '_' + db + '_ASV.csv'\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as input_file, open(\"_PR2_eDNA.csv\", \"a\") as output_file:\n",
    "            output_file.write(input_file.read())\n",
    "\n",
    "with open(\"_PR2_eDNA.csv\", \"r\") as input_file, open(\"PR2_eDNA.csv\", \"w\") as output_file:\n",
    "    output_file.write(\"counts,cluster,accession,accession,evalue,bitscore,alignment_length,percentage_identity,taxonomic_annotation\\n\")\n",
    "    for line in input_file:\n",
    "        if not line.startswith(\"#\"):\n",
    "            output_file.write(line.replace(\";\", \",\").replace(\"|\", \",\"))\n",
    "\n",
    "#shutil.copy(\"PR2_eDNA.csv\", os.path.join(current_dir, \"PR2_eDNA.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
